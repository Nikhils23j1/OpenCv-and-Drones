{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:3493: UserWarning: Config option `use_matplotlib` not recognized by `IPCompleter`.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.1\n"
     ]
    }
   ],
   "source": [
    "# Change the Jupyter kernel's executable\n",
    "%config IPCompleter.use_jedi = False\n",
    "%config IPCompleter.use_matplotlib = False\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "# Change the kernel's executable to the virtual environment's Python\n",
    "import sys\n",
    "sys.executable = '/Users/nikhil/My Computer/A Year 2/DSA/python/image processing/Assignment 2/venv/bin/python3'\n",
    "\n",
    "# Verify the Python executable path\n",
    "sys.executable\n",
    "\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coefficients(mtx, dist, path):\n",
    "    \"\"\" Save the camera matrix and the distortion coefficients to given path/file. \"\"\"\n",
    "    cv_file = cv2.FileStorage(path, cv2.FILE_STORAGE_WRITE)\n",
    "    cv_file.write(\"K\", mtx)\n",
    "    cv_file.write(\"D\", dist)\n",
    "    # note you *release* you don't close() a FileStorage object\n",
    "    cv_file.release()\n",
    "def load_coefficients(path):\n",
    "    \"\"\" Loads camera matrix and distortion coefficients. \"\"\"\n",
    "    # FILE_STORAGE_READ\n",
    "    cv_file = cv2.FileStorage(path, cv2.FILE_STORAGE_READ)\n",
    "\n",
    "    # note we also have to specify the type to retrieve other wise we only get a\n",
    "    # FileNode object back instead of a matrix\n",
    "    camera_matrix = cv_file.getNode(\"K\").mat()\n",
    "    dist_matrix = cv_file.getNode(\"D\").mat()\n",
    "\n",
    "    cv_file.release()\n",
    "    return [camera_matrix, dist_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18100203605511817, array([[1.54796962e+03, 0.00000000e+00, 9.29596692e+02],\n",
      "       [0.00000000e+00, 1.54394442e+03, 5.69504179e+02],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]), array([[ 0.06852545,  0.02704767,  0.00174481, -0.00469339, -0.24183901]]), (array([[ 0.03566208],\n",
      "       [ 0.35416381],\n",
      "       [-1.53153193]]),), (array([[ 52.49556764],\n",
      "       [108.13678615],\n",
      "       [508.43002555]]),)]\n",
      "Calibration successful.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Global variables to store calibration results\n",
    "calibration_results = None\n",
    "\n",
    "def calibrate(dirpath, prefix, image_format, square_size, width=9, height=6):\n",
    "    \"\"\" Apply camera calibration operation for images in the given directory path. \"\"\"\n",
    "    global calibration_results\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(8,6,0)\n",
    "    objp = np.zeros((height*width, 3), np.float32)\n",
    "    objp[:, :2] = np.mgrid[0:width, 0:height].T.reshape(-1, 2)\n",
    "\n",
    "    objp = objp * square_size\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = []  # 3d point in real-world space\n",
    "    imgpoints = []  # 2d points in the image plane.\n",
    "\n",
    "    if dirpath[-1:] == '/':\n",
    "        dirpath = dirpath[:-1]\n",
    "\n",
    "    images = glob.glob(dirpath + '/' + prefix + '*.' + image_format)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (width, height), None)\n",
    "\n",
    "        # If found, add object points, image points (after refining them)\n",
    "        if ret:\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "            imgpoints.append(corners2)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (width, height), corners2, ret)\n",
    "\n",
    "            # Calibrate camera for each image\n",
    "            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera([objp], [corners2], gray.shape[::-1], None, None)\n",
    "\n",
    "            # Store the calibration results in the global variable\n",
    "            calibration_results = [ret, mtx, dist, rvecs, tvecs]\n",
    "\n",
    "    return calibration_results\n",
    "\n",
    "# Example usage\n",
    "dirpath = '/Users/nikhil/My Computer/A Year 2/DSA/python/image processing/calibratediary'\n",
    "prefix = 'img'\n",
    "image_format = 'png'\n",
    "square_size = 25.0  # Adjust the square size based on your calibration pattern\n",
    "width = 10\n",
    "height = 7\n",
    "\n",
    "calibration_results = calibrate(dirpath, prefix, image_format, square_size, width, height)\n",
    "print(calibration_results)\n",
    "\n",
    "save_coefficients(calibration_results[1], calibration_results[2], \"calibration.yaml\")\n",
    "\n",
    "# Check if calibration was successful before unpacking results\n",
    "if calibration_results is not None:\n",
    "    ret, mtx, dist, rvecs, tvecs = calibration_results\n",
    "    # Access calibration results if needed\n",
    "    print(\"Calibration successful.\")\n",
    "else:\n",
    "    print(\"Calibration failed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to capture a frame. Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Load the calibration coefficients later if needed\n",
    "loaded_coefficients = load_coefficients(\"calibration.yaml\")\n",
    "\n",
    "# Assign the loaded coefficients to mtx and dist\n",
    "mtx, dist = loaded_coefficients\n",
    "\n",
    "from cv2 import aruco\n",
    "\n",
    "def track(matrix_coefficients, distortion_coefficients):\n",
    "    cap = cv2.VideoCapture(0)  # Change camera index if needed\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    # Define ArUco dictionary and parameters\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Failed to capture a frame. Exiting...\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect markers\n",
    "        markerCorners, markerIds, rejectedCandidates = detector.detectMarkers(gray)\n",
    "\n",
    "        if markerIds is not None and len(markerIds) > 0:\n",
    "            # Draw detected markers\n",
    "            frame = cv2.aruco.drawDetectedMarkers(frame, markerCorners, markerIds)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        key = cv2.waitKey(3) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the track function with calibration results\n",
    "track(mtx,dist)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage with calibration results\u001b[39;00m\n\u001b[1;32m     53\u001b[0m mtx, dist \u001b[38;5;241m=\u001b[39m load_coefficients(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mmarkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mmarkers\u001b[0;34m(matrix_coefficients, distortion_coefficients)\u001b[0m\n\u001b[1;32m     27\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maruco\u001b[38;5;241m.\u001b[39mdrawDetectedMarkers(frame, markerCorners, markerIds)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate pose information for the detected marker\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m rvec, tvec, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maruco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimatePoseSingleMarkers\u001b[49m(markerCorners, \u001b[38;5;241m0.05\u001b[39m, matrix_coefficients, distortion_coefficients)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Draw pose axes\u001b[39;00m\n\u001b[1;32m     33\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maruco\u001b[38;5;241m.\u001b[39mdrawAxis(frame, matrix_coefficients, distortion_coefficients, rvec, tvec, \u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def markers(matrix_coefficients, distortion_coefficients):\n",
    "    cap = cv2.VideoCapture(0)  # Change camera index if needed\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    # Define ArUco dictionary and parameters\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Failed to capture a frame. Exiting...\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect markers\n",
    "        markerCorners, markerIds, rejectedCandidates = detector.detectMarkers(gray)\n",
    "\n",
    "        if markerIds is not None and len(markerIds) > 0:\n",
    "            # Draw bounding box for the detected marker\n",
    "            frame = cv2.aruco.drawDetectedMarkers(frame, markerCorners, markerIds)\n",
    "\n",
    "            # Calculate pose information for the detected marker\n",
    "            rvec, tvec, _ = cv2.aruco.estimatePoseSingleMarkers(markerCorners, 0.05, matrix_coefficients, distortion_coefficients)\n",
    "\n",
    "            # Draw pose axes\n",
    "            frame = cv2.aruco.drawAxis(frame, matrix_coefficients, distortion_coefficients, rvec, tvec, 0.1)\n",
    "\n",
    "            # Compute the center of the marker\n",
    "            xc = int((markerCorners[0][0][0][0] + markerCorners[0][0][2][0]) / 2)\n",
    "            yc = int((markerCorners[0][0][0][1] + markerCorners[0][0][2][1]) / 2)\n",
    "\n",
    "            # Display the center coordinates (xc, yc)\n",
    "            print(f\"Center Coordinates: ({xc}, {yc})\")\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        key = cv2.waitKey(3) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage with calibration results\n",
    "mtx, dist = load_coefficients(\"calibration.yaml\")\n",
    "markers(mtx, dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m loaded_coefficients \u001b[38;5;241m=\u001b[39m load_coefficients(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m mtx, dist \u001b[38;5;241m=\u001b[39m loaded_coefficients\n\u001b[0;32m---> 48\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmarkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     xc, yc, translation_vector, rotation_vector \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mmarkers\u001b[0;34m(s, mtx, dist)\u001b[0m\n\u001b[1;32m     16\u001b[0m cv2\u001b[38;5;241m.\u001b[39maruco\u001b[38;5;241m.\u001b[39mdrawDetectedMarkers(img, markerCorners, markerIds)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Estimate pose of the detected marker\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m rvecs, tvecs,_\u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maruco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimatePoseSingleMarkers\u001b[49m(markerCorners, \u001b[38;5;241m0.02\u001b[39m, mtx, dist)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Draw pose axes for each detected marker\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(markerIds)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def markers(s, mtx, dist):\n",
    "    # Load the image\n",
    "    img = cv2.imread(s)\n",
    "\n",
    "    # Define ArUco dictionary and parameters\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "    # Detect markers in the image\n",
    "    markerCorners, markerIds, rejectedCandidates = detector.detectMarkers(img)\n",
    "\n",
    "    # Check if any markers are detected\n",
    "    if markerIds is not None and len(markerIds) > 0:\n",
    "        # Draw bounding box for the detected marker\n",
    "        cv2.aruco.drawDetectedMarkers(img, markerCorners, markerIds)\n",
    "\n",
    "        # Estimate pose of the detected marker\n",
    "        rvecs, tvecs,_= cv2.aruco.estimatePoseSingleMarkers(markerCorners, 0.02, mtx, dist)\n",
    "\n",
    "        # Draw pose axes for each detected marker\n",
    "        for i in range(len(markerIds)):\n",
    "            cv2.aruco.drawAxis(img, mtx, dist, rvecs[i], tvecs[i], 0.01)\n",
    "\n",
    "        # Get translation and rotation vectors for the first marker (assuming only one marker is present)\n",
    "        translation_vector = tvecs[0].flatten()\n",
    "        rotation_vector = rvecs[0].flatten()\n",
    "\n",
    "        # Compute center of the marker\n",
    "        xc, yc = np.mean(markerCorners[0][0], axis=0)\n",
    "\n",
    "        # Display the image with the bounding box and pose axes\n",
    "        cv2.imshow('Detected Markers', img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Return relevant information\n",
    "        return xc, yc, translation_vector, rotation_vector\n",
    "    else:\n",
    "        print(\"No markers detected in the image.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "image_path = 'img0.png'\n",
    "\n",
    "loaded_coefficients = load_coefficients(\"calibration.yaml\")\n",
    "mtx, dist = loaded_coefficients\n",
    "result = markers(image_path, mtx, dist)\n",
    "\n",
    "if result is not None:\n",
    "    xc, yc, translation_vector, rotation_vector = result\n",
    "    print(\"Center of Marker (xc, yc):\", xc, yc)\n",
    "    print(\"Translation Vector:\", translation_vector)\n",
    "    print(\"Rotation Vector:\", rotation_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m loaded_coefficients \u001b[38;5;241m=\u001b[39m load_coefficients(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m mtx, dist \u001b[38;5;241m=\u001b[39m loaded_coefficients\n\u001b[0;32m---> 53\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmarkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     xc, yc, translation_vector, rotation_vector \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mmarkers\u001b[0;34m(s, mtx, dist)\u001b[0m\n\u001b[1;32m     20\u001b[0m cv2\u001b[38;5;241m.\u001b[39maruco\u001b[38;5;241m.\u001b[39mdrawDetectedMarkers(img, markerCorners, markerIds)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Estimate pose of the detected marker\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m rvecs, tvecs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maruco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimatePoseSingleMarkers\u001b[49m(markerCorners, \u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, mtx, dist, cameraMatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, markerLength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, parameters\u001b[38;5;241m=\u001b[39mparameters)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Draw pose axes for each detected marker\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(markerIds)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.aruco' has no attribute 'estimatePoseSingleMarkers'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def markers(s, mtx, dist):\n",
    "    # Load the image\n",
    "    img = cv2.imread(s)\n",
    "\n",
    "    # Define ArUco dictionary and parameters\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    parameters.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "    # Detect markers in the image\n",
    "    markerCorners, markerIds, rejectedCandidates = detector.detectMarkers(img)\n",
    "\n",
    "    # Check if any markers are detected\n",
    "    if markerIds is not None and len(markerIds) > 0:\n",
    "        # Draw bounding box for the detected marker\n",
    "        cv2.aruco.drawDetectedMarkers(img, markerCorners, markerIds)\n",
    "\n",
    "        # Estimate pose of the detected marker\n",
    "        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(markerCorners, 0.02, None, None, mtx, dist, cameraMatrix=None, markerLength=1.0, parameters=parameters)\n",
    "\n",
    "        # Draw pose axes for each detected marker\n",
    "        for i in range(len(markerIds)):\n",
    "            cv2.aruco.drawAxis(img, mtx, dist, rvecs[i], tvecs[i], 0.01)\n",
    "\n",
    "        # Get translation and rotation vectors for the first marker (assuming only one marker is present)\n",
    "        translation_vector = tvecs[0].flatten()\n",
    "        rotation_vector = rvecs[0].flatten()\n",
    "\n",
    "        # Compute center of the marker\n",
    "        xc, yc = np.mean(markerCorners[0][0], axis=0)\n",
    "\n",
    "        # Display the image with the bounding box and pose axes\n",
    "        cv2.imshow('Detected Markers', img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Return relevant information\n",
    "        return xc, yc, translation_vector, rotation_vector\n",
    "    else:\n",
    "        print(\"No markers detected in the image.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "image_path = 'img0.png'\n",
    "\n",
    "# Assuming the load_coefficients function is defined and returns loaded_coefficients\n",
    "loaded_coefficients = load_coefficients(\"calibration.yaml\")\n",
    "mtx, dist = loaded_coefficients\n",
    "result = markers(image_path, mtx, dist)\n",
    "\n",
    "if result is not None:\n",
    "    xc, yc, translation_vector, rotation_vector = result\n",
    "    print(\"Center of Marker (xc, yc):\", xc, yc)\n",
    "    print(\"Translation Vector:\", translation_vector)\n",
    "    print(\"Rotation Vector:\", rotation_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
